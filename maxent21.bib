
@inproceedings{aggarwal2001_SurprisingBehaviorDistance,
  title = {On the {{Surprising Behavior}} of {{Distance Metrics}} in {{High Dimensional Space}}},
  booktitle = {Database {{Theory}} \textemdash{} {{ICDT}} 2001},
  author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
  editor = {{Van den Bussche}, Jan and Vianu, Victor},
  year = {2001},
  pages = {420--434},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44503-X_27},
  abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1 norm) is consistently more preferable than the Euclidean distance metric L(2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
  file = {/home/ert/Zotero/storage/X3ZZEQ5E/Aggarwal et al. - 2001 - On the Surprising Behavior of Distance Metrics in .pdf},
  isbn = {978-3-540-44503-6},
  keywords = {Confusion Matrice,Distance Metrics,High Dimensional Space,Manhattan Distance,Query Point},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@techreport{allenby2005_HierarchicalBayesModels,
  title = {Hierarchical {{Bayes Models}}: {{A Practitioners Guide}}},
  shorttitle = {Hierarchical {{Bayes Models}}},
  author = {Allenby, Greg M. and Rossi, Peter E. and McCulloch, Robert E.},
  year = {2005},
  month = jan,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.655541},
  abstract = {Hierarchical Bayes models free researchers from computational constraints and allow for the development of more realistic models of buyer behavior and decision making. Moreover, this freedom enables exploration of marketing problems that have proven elusive over the years, such as models for advertising ROI, sales force effectiveness, and similarly complex problems that often involve simultaneity.  The promise of Bayesian statistical methods lies in the ability to deal with these complex problems, but the very complexity of the problems creates a significant challenge to both researchers and practitioners.   We illustrate the promise of HB models and provide an introduction to their computation.},
  file = {/home/ert/Zotero/storage/GV9E5I7C/papers.html},
  keywords = {bayes,conjoint,Hiearchical models,survey research},
  language = {en},
  number = {ID 655541},
  type = {{{SSRN Scholarly Paper}}}
}

@article{bayarriComputerModelValidation2007,
  title = {Computer Model Validation with Functional Output},
  author = {Bayarri, M. J. and Walsh, D. and Berger, J. O. and Cafeo, J. and {Garcia-Donato}, G. and Liu, F. and Palomo, J. and Parthasarathy, R. J. and Paulo, R. and Sacks, J.},
  year = {2007},
  month = oct,
  volume = {35},
  issn = {0090-5364},
  doi = {10.1214/009053607000000163},
  journal = {Ann. Statist.},
  number = {5}
}

@book{bishop1995neural,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M},
  year = {2006},
  publisher = {Springer},
  isbn = {978-0387-31073-2}
}

@incollection{cadzow1987spectral,
  title = {Spectral Analysis},
  booktitle = {Handbook of Digital Signal Processing},
  author = {Cadzow, James A.},
  year = {1987},
  pages = {701--740},
  publisher = {{Elsevier}},
  isbn = {978-0-08-050780-4}
}

@article{callies2008_CalibrationUncertaintyAnalysis,
  title = {Calibration and Uncertainty Analysis of a Simple Model of Silica-Limited Diatom Growth in the {{Elbe River}}},
  author = {Callies, U. and Scharfe, M. and Ratto, M.},
  year = {2008},
  month = may,
  volume = {213},
  pages = {229--244},
  issn = {0304-3800},
  doi = {10.1016/j.ecolmodel.2007.12.015},
  abstract = {A simple Lagrangian water quality model was designed to investigate the hypothesis of sporadic silica limitations of diatom growth in the lower Elbe River in Germany. For each fluid parcel a limited reservoir of silica was specified to be consumed by diatoms. The model's simplicity notwithstanding, a set of six selected model parameters could not be fully identified from existing observations at one station. After the introduction of prior knowledge of the ranges of meaningful parameter values, calibration of the over-parameterised model manifested itself primarily in the generation of posterior parameter covariances. Estimations of the covariance matrix based on (a) second order partial derivatives of a quadratic cost function at its optimum and (b) Monte Carlo simulations exploring the whole space of parameter values gave consistent results. Diagonalisation of the covariance matrix yielded two linear parameter combinations that were most effectively controlled by data from periods with and without lack of silica, respectively. The two parameter combinations were identified as the essential inputs that govern the successful simulation of intermittently decreasing chlorophyll a concentrations in summer. A satisfactory simulation of the pronounced chlorophyll a minimum in spring, by contrast, was found to be beyond the means of the simple model.},
  file = {/home/ert/Zotero/storage/MTP7X6HG/Callies et al. - 2008 - Calibration and uncertainty analysis of a simple m.pdf;/home/ert/Zotero/storage/RPWVIBKV/S0304380007006503.html},
  journal = {Ecological Modelling},
  keywords = {Elbe River,GLUE,Model over-parameterisation,Model uncertainty analysis,Principal component analysis,Silica limitation},
  language = {en},
  number = {2}
}

@article{callies2021_ParameterDependencesArising,
  title = {Parameter {{Dependences Arising}} from {{Calibration}} of a {{Riverine Diatom Model}} - {{Representation}} in {{Terms}} of {{Posterior Conditional Distributions}}},
  author = {Callies, Ulrich and Albert, Christopher G. and {von Toussaint}, Udo},
  year = {2021},
  month = jun,
  doi = {10.21203/rs.3.rs-445734/v1},
  abstract = {We address the analysis and proper representation of posterior dependence among parameters obtained from model calibration. A simple water quality model for the Elbe River (Germany) is referred to as an example. The joint posterior distribution of six model parameters is estimated by Markov Chain Monte Carlo sampling based on a quadratic likelihood function. The estimated distribution shows to which extent model parameters are controlled by observations, highlighting issues that cannot be settled unless more information becomes available. In our example, some vagueness occurs due to problems in distinguishing between the effects of either growth limitation by lack of silica or a temperature dependent algal loss rate. Knowing such indefiniteness of the model structure is crucial when the model is to be used in support of management options. Bayesian network technology can be employed to convey this information in a transparent way.},
  annotation = {(submitted)},
  journal = {Environmental Modeling \& Assessment}
}

@article{campbell2006_SensitivityAnalysisWhen,
  title = {Sensitivity Analysis When Model Outputs Are Functions},
  author = {Campbell, Katherine and McKay, Michael D. and Williams, Brian J.},
  year = {2006},
  month = oct,
  volume = {91},
  pages = {1468--1472},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2005.11.049},
  abstract = {When outputs of computational models are time series or functions of other continuous variables like distance, angle, etc., it can be that primary interest is in the general pattern or structure of the curve. In these cases, model sensitivity and uncertainty analysis focuses on the effect of model input choices and uncertainties in the overall shapes of such curves. We explore methods for characterizing a set of functions generated by a series of model runs for the purpose of exploring relationships between these functions and the model inputs.},
  file = {/home/ert/Zotero/storage/C5QW9QS9/Campbell et al. - 2006 - Sensitivity analysis when model outputs are functi.pdf;/home/ert/Zotero/storage/PMDPGAII/S0951832005002565.html},
  journal = {Reliability Engineering \& System Safety},
  keywords = {Basis functions,Functional data analysis,Functional sensitivity analysis},
  language = {en},
  number = {10},
  series = {The {{Fourth International Conference}} on {{Sensitivity Analysis}} of {{Model Output}} ({{SAMO}} 2004)}
}

@article{christenMarkovChainMonte2005,
  title = {Markov {{Chain Monte Carlo Using}} an {{Approximation}}},
  author = {Christen, J. Andr{\'e}s and Fox, Colin},
  year = {2005},
  volume = {14},
  pages = {795--810},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]}},
  issn = {1061-8600},
  abstract = {This article presents a method for generating samples from an unnormalized posterior distribution f({$\cdot$}) using Markov chain Monte Carlo (MCMC) in which the evaluation of f({$\cdot$}) is very difficult or computationally demanding. Commonly, a less computationally demanding, perhaps local, approximation to f({$\cdot$}) is available, say \$f\_\{x\}\^\{\textbackslash ast\}\textbackslash{} \$({$\cdot$}). An algorithm is proposed to generate an MCMC that uses such an approximation to calculate acceptance probabilities at each step of a modified Metropolis\textendash Hastings algorithm. Once a proposal is accepted using the approximation, f({$\cdot$}) is calculated with full precision ensuring convergence to the desired distribution. We give sufficient conditions for the algorithm to converge to f({$\cdot$}) and give both theoretical and practical justifications for its usage. Typical applications are in inverse problems using physical data models where computing time is dominated by complex model simulation. We outline Bayesian inference and computing for inverse problems. A stylized example is given of recovering resistor values in a network from electrical measurements made at the boundary. Although this inverse problem has appeared in studies of underground reservoirs, it has primarily been chosen for pedagogical value because model simulation has precisely the same computational structure as a finite element method solution of the complete electrode model used in conductivity imaging, or "electrical impedance tomography." This example shows a dramatic decrease in CPU time, compared to a standard Metropolis\textemdash Hastings algorithm.},
  file = {/home/ert/Zotero/storage/3PJ2KEM9/Christen und Fox - 2005 - Markov Chain Monte Carlo Using an Approximation.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  number = {4}
}

@article{clearyCalibrateEmulateSample2021,
  title = {Calibrate, Emulate, Sample},
  author = {Cleary, Emmet and {Garbuno-Inigo}, Alfredo and Lan, Shiwei and Schneider, Tapio and Stuart, Andrew M.},
  year = {2021},
  month = jan,
  volume = {424},
  pages = {109716},
  issn = {00219991},
  doi = {10.1016/j.jcp.2020.109716},
  journal = {Journal of Computational Physics},
  language = {en}
}

@article{conradAcceleratingAsymptoticallyExact2016,
  title = {Accelerating {{Asymptotically Exact MCMC}} for {{Computationally Intensive Models}} via {{Local Approximations}}},
  author = {Conrad, Patrick R. and Marzouk, Youssef M. and Pillai, Natesh S. and Smith, Aaron},
  year = {2016},
  month = oct,
  volume = {111},
  pages = {1591--1607},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2015.1096787},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {516}
}

@book{DesignAnalysisComputer2018,
  title = {The Design and Analysis of Computer Experiments},
  year = {2018},
  publisher = {{Springer Science+Business Media, LLC}},
  address = {{New York, NY}},
  isbn = {978-1-4939-8845-7}
}

@article{dose2006_BayesianEstimateNewtonian,
  title = {Bayesian Estimate of the {{Newtonian}} Constant of Gravitation},
  author = {Dose, Volker},
  year = {2006},
  month = nov,
  volume = {18},
  pages = {176--182},
  publisher = {{IOP Publishing}},
  issn = {0957-0233},
  doi = {10.1088/0957-0233/18/1/022},
  abstract = {Bayesian probability theory is employed to derive robust, outlier tolerant methods for the estimation of a quantity and the determination of the uncertainty associated with this estimation given a set of data. The procedure is applied to the estimate of the Newtonian constant of gravitation G yielding This value is in good agreement with the recently published value of the 2002 CODATA adjustment but offers a four-fold reduced, rigorously calculated uncertainty. The uncertainty reflects\textemdash unlike results from the conventional least squares analysis\textemdash the quoted uncertainties of the data as well as the data scatter.},
  file = {/home/ert/Zotero/storage/7CSBYB6N/Dose - 2006 - Bayesian estimate of the Newtonian constant of gra.pdf},
  journal = {Meas. Sci. Technol.},
  language = {en},
  number = {1}
}

@article{erikssonTodoNipsScalable2020,
  title = {Todo Nips {{Scalable Global Optimization}} via {{Local Bayesian Optimization}}},
  author = {Eriksson, David and Pearce, Michael and Gardner, Jacob R. and Turner, Ryan and Poloczek, Matthias},
  year = {2020},
  month = feb,
  abstract = {Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the \$\textbackslash texttt\{TuRBO\}\$ algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that \$\textbackslash texttt\{TuRBO\}\$ outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.},
  archiveprefix = {arXiv},
  eprint = {1910.01739},
  eprinttype = {arxiv},
  journal = {arXiv:1910.01739 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{ferLinkingBigModels2018,
  title = {Linking Big Models to Big Data: Efficient Ecosystem Model Calibration through {{Bayesian}} Model Emulation},
  shorttitle = {Linking Big Models to Big Data},
  author = {Fer, Istem and Kelly, Ryan and Moorcroft, Paul R. and Richardson, Andrew D. and Cowdery, Elizabeth M. and Dietze, Michael C.},
  year = {2018},
  month = oct,
  volume = {15},
  pages = {5801--5830},
  issn = {1726-4189},
  doi = {10.5194/bg-15-5801-2018},
  abstract = {Abstract. Data-model integration plays a critical role in assessing and improving our capacity to predict ecosystem dynamics. Similarly, the ability to attach quantitative statements of uncertainty around model forecasts is crucial for model assessment and interpretation and for setting field research priorities. Bayesian methods provide a rigorous data assimilation framework for these applications, especially for problems with multiple data constraints. However, the Markov chain Monte Carlo (MCMC) techniques underlying most Bayesian calibration can be prohibitive for computationally demanding models and large datasets. We employ an alternative method, Bayesian model emulation of sufficient statistics, that can approximate the full joint posterior density, is more amenable to parallelization, and provides an estimate of parameter sensitivity. Analysis involved informative priors constructed from a meta-analysis of the primary literature and specification of both model and data uncertainties, and it introduced novel approaches to autocorrelation corrections on multiple data streams and emulating the sufficient statistics surface. We report the integration of this method within an ecological workflow management software, Predictive Ecosystem Analyzer (PEcAn), and its application and validation with two process-based terrestrial ecosystem models: SIPNET and ED2. In a test against a synthetic dataset, the emulator was able to retrieve the true parameter values. A comparison of the emulator approach to standard brute-force MCMC involving multiple data constraints showed that the emulator method was able to constrain the faster and simpler SIPNET model's parameters with comparable performance to the brute-force approach but reduced computation time by more than 2 orders of magnitude. The emulator was then applied to calibration of the ED2 model, whose complexity precludes standard (brute-force) Bayesian data assimilation techniques. Both models are constrained after assimilation of the observational data with the emulator method, reducing the uncertainty around their predictions. Performance metrics showed increased agreement between model predictions and data. Our study furthers efforts toward reducing model uncertainties, showing that the emulator method makes it possible to efficiently calibrate complex models.},
  journal = {Biogeosciences},
  language = {en},
  number = {19}
}

@article{flexer2015_ChoosingLpNorms,
  title = {Choosing Lp Norms in High-Dimensional Spaces Based on Hub Analysis},
  author = {Flexer, Arthur and Schnitzer, Dominik},
  year = {2015},
  month = dec,
  volume = {169},
  pages = {281--287},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2014.11.084},
  abstract = {The hubness phenomenon is a recently discovered aspect of the curse of dimensionality. Hub objects have a small distance to an exceptionally large number of data points while anti-hubs lie far from all other data points. A closely related problem is the concentration of distances in high-dimensional spaces. Previous work has already advocated the use of fractional {$\mathscr{l}$}p norms instead of the ubiquitous Euclidean norm to avoid the negative effects of distance concentration. However, which exact fractional norm to use is a largely unsolved problem. The contribution of this work is an empirical analysis of the relation of different {$\mathscr{l}$}p norms and hubness. We propose an unsupervised approach for choosing an lp norm which minimizes hubs while simultaneously maximizing nearest neighbor classification. Our approach is evaluated on seven high-dimensional data sets and compared to three approaches that re-scale distances to avoid hubness.},
  file = {/home/ert/Zotero/storage/I8NMFB6F/Flexer and Schnitzer - 2015 - Choosing â„“p norms in high-dimensional spaces based.pdf;/home/ert/Zotero/storage/YMN3FNUK/S0925231215004336.html},
  journal = {Neurocomputing},
  keywords = {Concentration of distances,Fractional norms,High-dimensional data analysis,Hubness},
  language = {en},
  series = {Learning for {{Visual Semantic Understanding}} in {{Big Data}}}
}

@article{gongAdaptiveSurrogateModelingbased2017,
  title = {An Adaptive Surrogate Modeling-Based Sampling Strategy for Parameter Optimization and Distribution Estimation ({{ASMO}}-{{PODE}})},
  author = {Gong, Wei and Duan, Qingyun},
  year = {2017},
  month = sep,
  volume = {95},
  pages = {61--75},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2017.05.005},
  journal = {Environmental Modelling \& Software},
  language = {en}
}

@misc{gpy2014,
  title = {{{GPy}}: {{A Gaussian}} Process Framework in Python},
  author = {{GPy}},
  year = {since 2012}
}

@article{haarioAdaptiveMetropolisAlgorithm2001,
  title = {An {{Adaptive Metropolis Algorithm}}},
  author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
  year = {2001},
  volume = {7},
  pages = {223--242},
  publisher = {{International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.2307/3318737},
  abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
  file = {/home/ert/Zotero/storage/BQZZZ8CK/Haario et al. - 2001 - An Adaptive Metropolis Algorithm.pdf;/home/ert/Zotero/storage/IYV74NPS/accept.html},
  journal = {Bernoulli},
  number = {2}
}

@article{lebel2019_StatisticalInverseIdentification,
  title = {Statistical Inverse Identification for Nonlinear Train Dynamics Using a Surrogate Model in a {{Bayesian}} Framework},
  author = {Lebel, D. and Soize, C. and F{\"u}nfschilling, C. and Perrin, G.},
  year = {2019},
  month = oct,
  volume = {458},
  pages = {158--176},
  issn = {0022-460X},
  doi = {10.1016/j.jsv.2019.06.024},
  abstract = {This paper presents a Bayesian calibration method for a simulation-based model with stochastic functional input and output. The originality of the method lies in an adaptation involving the representation of the likelihood function by a Gaussian process surrogate model, to cope with the high computational cost of the simulation, while avoiding the surrogate modeling of the functional output. The adaptation focuses on taking into account the uncertainty introduced by the use of a surrogate model when estimating the parameters posterior probability distribution by MCMC. To this end, trajectories of the random surrogate model of the likelihood function are drawn and injected in the MCMC algorithm. An application on a train suspension monitoring case is presented.},
  file = {/home/ert/Zotero/storage/9QP6NXYB/Lebel et al. - 2019 - Statistical inverse identification for nonlinear t.pdf;/home/ert/Zotero/storage/DYW4DBA7/S0022460X19303633.html},
  journal = {Journal of Sound and Vibration},
  keywords = {Bayesian calibration,High-speed train dynamics,Statistical inverse problem,Surrogate model,Uncertainty quantification},
  language = {en}
}

@inproceedings{mahendranAdaptiveMCMCBayesian2012,
  title = {Adaptive {{MCMC}} with {{Bayesian Optimization}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Mahendran, Nimalan and Wang, Ziyu and Hamze, Firas and Freitas, Nando De},
  year = {2012},
  month = mar,
  pages = {751--760},
  publisher = {{PMLR}},
  abstract = {This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitat...},
  language = {en}
}

@article{manekFastRegressionTritium2021,
  title = {Fast {{Regression}} of the {{Tritium Breeding Ratio}} in {{Fusion Reactors}}},
  author = {M{\'a}nek, Petr and Van Goffrier, Graham and Gopakumar, Vignesh and Nikolaou, Nikolaos and Shimwell, Jonathan and Waldmann, Ingo},
  year = {2021},
  month = apr,
  abstract = {The tritium breeding ratio (TBR) is an essential quantity for the design of modern and next-generation D-T fueled nuclear fusion reactors. Representing the ratio between tritium fuel generated in breeding blankets and fuel consumed during reactor runtime, the TBR depends on reactor geometry and material properties in a complex manner. In this work, we explored the training of surrogate models to produce a cheap but high-quality approximation for a Monte Carlo TBR model in use at the UK Atomic Energy Authority. We investigated possibilities for dimensional reduction of its feature space, reviewed 9 families of surrogate models for potential applicability, and performed hyperparameter optimisation. Here we present the performance and scaling properties of these models, the fastest of which, an artificial neural network, demonstrated \$R\^2=0.985\$ and a mean prediction time of \$0.898\textbackslash{} \textbackslash mu\textbackslash mathrm\{s\}\$, representing a relative speedup of \$8\textbackslash cdot 10\^6\$ with respect to the expensive MC model. We further present a novel adaptive sampling algorithm, Quality-Adaptive Surrogate Sampling, capable of interfacing with any of the individually studied surrogates. Our preliminary testing on a toy TBR theory has demonstrated the efficacy of this algorithm for accelerating the surrogate modelling process.},
  archiveprefix = {arXiv},
  eprint = {2104.04026},
  eprinttype = {arxiv},
  journal = {arXiv:2104.04026 [physics]},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics},
  primaryclass = {physics}
}

@article{newman1996_ModelReductionKarhunenLoeve,
  title = {Model {{Reduction}} via the {{Karhunen}}-{{Loeve Expansion Part I}}: {{An Exposition}}},
  shorttitle = {Model {{Reduction}} via the {{Karhunen}}-{{Loeve Expansion Part I}}},
  author = {Newman, Andrew J.},
  year = {1996},
  abstract = {In formulating mathematical models for dynamical systems, obtaining a high degree of qualitative correctness (i.e. predictive capability) may not be the only objective. The model must be useful for its intended application,and models of reduced complexity are attractive in many cases.{$<$}p{$>$}In Part I of this paper we provide an exposition of some techniques that are useful in finding models of reduced complexity for dynamical systems involving flows. The material presented here is not new. The techniques we discussare based on classical theory such as the Karhunen-Loeve expansion and the method of Galerkin, and the more recent concept of "coherent structures." They have been heavily exploited in a wide range of areas in science and engineering.{$<$}p{$>$}The attempt here is to present this collectionof important methods and ideas together, at a high level of detail, in coherent form, and in the context of model reduction for simulation and control. In this manner we lead in to Part II which illustrates theirusefulness in model reduction by applying them to some elementary examples of distributed parameter systems which are related to processes found in semiconductor manufacturing.},
  annotation = {Accepted: 2007-05-23T10:01:31Z},
  file = {/home/ert/Zotero/storage/XU6ML3V2/Newman - 1996 - Model Reduction via the Karhunen-Loeve Expansion P.pdf;/home/ert/Zotero/storage/SPZAIMVD/5751.html},
  language = {en\_US}
}

@article{ohagan1978_CurveFittingOptimal,
  title = {Curve {{Fitting}} and {{Optimal Design}} for {{Prediction}}},
  author = {O'Hagan, A.},
  year = {1978},
  month = sep,
  volume = {40},
  pages = {1--24},
  publisher = {{John Wiley \& Sons, Ltd (10.1111)}},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1978.tb01643.x},
  abstract = {The optimal design problem is tackled in the framework of a new model and new objectives. A regression model is proposed in which the regression function is permitted to take any form over the space X of independent variables. The design objective is based on fitting a simplified function for prediction. The approach is Bayesian throughout. The new designs are more robust than conventional ones. They also avoid the need to limit artificially design points to a predetermined subset of X. New solutions are also offered for the problems of smoothing, curve fitting and the selection of regressor variables.},
  file = {/home/ert/Zotero/storage/ESTR99RX/O'Hagan - 1978 - Curve Fitting and Optimal Design for Prediction.pdf},
  journal = {J. R. Stat. Soc. Ser. B},
  keywords = {bayes inference,curve fitting,Gaussian process,localized regression model,multiple regression,multivariate regression,optimal design,quadratic loss,selection of variables,smoothing},
  number = {1}
}

@inproceedings{osborne2009_GaussianProcessesGlobal,
  title = {Gaussian {{Processes}} for {{Global Optimization}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {Osborne, Michael A and Garnett, Roman and Roberts, Stephen J},
  year = {2009},
  abstract = {We introduce a novel Bayesian approach to global optimization using Gaussian processes. We frame the optimization of both noisy and noiseless functions as sequential decision problems, and introduce myopic and non-myopic solutions to them. Here our solutions can be tailored to exactly the degree of confidence we require of them. The use of Gaussian processes allows us to benefit from the incorporation of prior knowledge about our objective function, and also from any derivative observations. Using this latter fact, we introduce an innovative method to combat conditioning problems. Our algorithm demonstrates a significant improvement over its competitors in overall performance across a wide range of canonical test problems.},
  file = {/home/ert/Zotero/storage/W36VHZRW/Osborne et al. - Gaussian Processes for Global Optimization.pdf},
  language = {en}
}

@article{perrin2020_AdaptiveCalibrationComputera,
  title = {Adaptive Calibration of a Computer Code with Time-Series Output},
  author = {Perrin, G.},
  year = {2020},
  month = apr,
  volume = {196},
  pages = {106728},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2019.106728},
  abstract = {Simulation plays a major role in the conception, the optimization and the certification of complex systems. Of particular interest here is the calibration of the parameters of computer models from high-dimensional physical observations. When the run times of these computer codes is high, this work focuses on the numerical challenges associated with the statistical inference. In particular, several adaptations of the Gaussian Process Regression (GPR) to the high-dimensional or functional output case are presented for the emulation of computer codes from limited data. Then, an adaptive procedure is detailed to minimize the calibration parameters uncertainty at the minimal computational cost. The proposed method is eventually applied to two applications that are based on dynamic simulators.},
  file = {/home/ert/Zotero/storage/3LIIQBM6/Perrin - 2020 - Adaptive calibration of a computer code with time-.pdf;/home/ert/Zotero/storage/AQ2ADT3N/S0951832018311232.html},
  journal = {Reliability Engineering \& System Safety},
  keywords = {Bayesian framework,Computer experiment,Dynamic simulator,Gaussian process,Multi-output},
  language = {en}
}

@article{pratola2013_FastSequentialComputer,
  title = {Fast {{Sequential Computer Model Calibration}} of {{Large Nonstationary Spatial}}-{{Temporal Processes}}},
  author = {Pratola, Matthew T. and Sain, Stephan R. and Bingham, Derek and Wiltberger, Michael and Rigler, E. Joshua},
  year = {2013},
  month = may,
  volume = {55},
  pages = {232--242},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2013.775897},
  abstract = {Computer models enable scientists to investigate real-world phenomena in a virtual laboratory using computer experiments. Statistical calibration enables scientists to incorporate field data in this analysis. However, the practical application is hardly straightforward for data structures such as spatial-temporal fields, which are usually large or not well represented by a stationary process model. We present a computationally efficient approach to estimating the calibration parameters using a criterion that measures discrepancy between the computer model output and field data. One can then construct empirical distributions for the calibration parameters and propose new computer model trials using sequential design. The approach is relatively simple to implement using existing algorithms and is able to estimate calibration parameters for large and nonstationary data. Supplementary R code is available online.},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2013.775897},
  file = {/home/ert/Zotero/storage/R993FZRG/00401706.2013.html},
  journal = {Technometrics},
  keywords = {Conditional simulation,Gaussian process,Likelihood ratio,Magnetosphere,Sequential design},
  number = {2}
}

@article{preuss2018_GlobalOptimizationEmploying,
  title = {Global {{Optimization Employing Gaussian Process}}-{{Based Bayesian Surrogates}}},
  author = {Preuss, Roland and {von Toussaint}, Udo},
  year = {2018},
  month = mar,
  volume = {20},
  pages = {201},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e20030201},
  abstract = {The simulation of complex physics models may lead to enormous computer running times. Since the simulations are expensive it is necessary to exploit the computational budget in the best possible manner. If for a few input parameter settings an output data set has been acquired, one could be interested in taking these data as a basis for finding an extremum and possibly an input parameter set for further computer simulations to determine it\textemdash a task which belongs to the realm of global optimization. Within the Bayesian framework we utilize Gaussian processes for the creation of a surrogate model function adjusted self-consistently via hyperparameters to represent the data. Although the probability distribution of the hyperparameters may be widely spread over phase space, we make the assumption that only the use of their expectation values is sufficient. While this shortcut facilitates a quickly accessible surrogate, it is somewhat justified by the fact that we are not interested in a full representation of the model by the surrogate but to reveal its maximum. To accomplish this the surrogate is fed to a utility function whose extremum determines the new parameter set for the next data point to obtain. Moreover, we propose to alternate between two utility functions\textemdash expected improvement and maximum variance\textemdash in order to avoid the drawbacks of each. Subsequent data points are drawn from the model function until the procedure either remains in the points found or the surrogate model does not change with the iteration. The procedure is applied to mock data in one and two dimensions in order to demonstrate proof of principle of the proposed approach.},
  file = {/home/ert/Zotero/storage/FHLPINRL/Preuss, von Toussaint - 2018 - Optimization Employing Gaussian Process-Based Surrogates.pdf},
  journal = {Entropy},
  keywords = {gaussian process,global optimization,parametric studies},
  number = {3}
}

@article{ranjan2016_InverseProblemTimeSeries,
  title = {Inverse {{Problem}} for a {{Time}}-{{Series Valued Computer Simulator}} via {{Scalarization}}},
  author = {Ranjan, Pritam and Thomas, Mark and Teismann, Holger and Mukhoti, Sujay},
  year = {2016},
  month = jun,
  volume = {6},
  pages = {528--544},
  publisher = {{Scientific Research Publishing}},
  doi = {10.4236/ojs.2016.63045},
  abstract = {For an expensive to evaluate computer simulator, even the estimate of the overall surface can be a challenging problem. In this paper, we focus on the estimation of the inverse solution, i.e., to find the set(s) of input combinations of the simulator that generates a pre-determined simulator output. Ranjan et al. [1] proposed an expected improvement criterion under a sequential design framework for the inverse problem with a scalar valued simulator. In this paper, we focus on the inverse problem for a time-series valued simulator. We have used a few simulated and two real examples for performance comparison.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  file = {/home/ert/Zotero/storage/JVMYSQPR/Ranjan et al. - 2016 - Inverse Problem for a Time-Series Valued Computer .pdf;/home/ert/Zotero/storage/CQSB9VZW/paperinformation.html},
  journal = {Open Journal of Statistics},
  language = {en},
  number = {3}
}

@book{rasmussenGaussianProcessesMachine2006,
  title = {Gaussian {{Processes}} for {{Machine Learning}}},
  author = {Rasmussen, C E and Williams, C K I},
  year = {2006},
  publisher = {{MIT Press}},
  issn = {0129-0657},
  doi = {10.1142/S0129065704001899},
  annotation = {\_eprint: 026218253X},
  file = {/home/ert/Zotero/storage/FKC9EHT3/Rasmussen, Williams - 2006 - Gaussian Processes for Machine Learning.pdf},
  isbn = {0-262-18253-X},
  pmid = {15112367}
}

@article{scharfe2009_SimpleLagrangianModel,
  title = {A Simple {{Lagrangian}} Model to Simulate Temporal Variability of Algae in the {{Elbe River}}},
  author = {Scharfe, Mirco and Callies, Ulrich and Bl{\"o}cker, Gerd and Petersen, Wilhelm and Schroeder, Friedhelm},
  year = {2009},
  month = sep,
  volume = {220},
  pages = {2173--2186},
  issn = {0304-3800},
  doi = {10.1016/j.ecolmodel.2009.04.048},
  abstract = {We present a five-year (1997\textendash 2001) numerical simulation of daily mean chlorophyll a concentrations at station Geesthacht Weir on the lower Elbe River (Germany) using an extremely simple Lagrangian model driven by (a) water discharge, global radiation, water temperature, and (b) silica observations at station Schmilka in the upper reach of the Elbe River. Notwithstanding the lack of many mechanistic details, the model is able to reproduce observed chlorophyll a variability surprisingly well, including a number of sharp valleys and ascents/descents in the observed time series. The model's success is based on the assumption of three key effects: prevailing light conditions, sporadic limitation of algal growth due to lack of silica and algae loss rates that increase above an empirically specified temperature threshold of 20\textdegree C. Trimmed-down model versions are studied to analyse the model's success in terms of these mechanisms.In each of the five years the model consistently fails, however, to properly simulate characteristic steep increases of chlorophyll a concentrations after pronounced spring minima. Curing this model deficiency by global model re-calibration was found to be impossible. However, suspension of silica consumption by algae for up to 10 days in spring is shown to serve as a successful placeholder for processes that are disregarded in the model but apparently play an important role in the distinctly marked period of model failure. For the remainder of the year the very simple model was found to be adequate.},
  file = {/home/ert/Zotero/storage/SI6YKBZ7/Scharfe et al. - 2009 - A simple Lagrangian model to simulate temporal var.pdf;/home/ert/Zotero/storage/X4TZPHH8/S0304380009003147.html},
  journal = {Ecological Modelling},
  keywords = {Algae dynamics,Elbe River,Physical forcing,Process interaction,Silica limitation},
  language = {en},
  number = {18}
}

@article{shahriariTakingHumanOut2016a,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = {2016},
  month = jan,
  volume = {104},
  pages = {148--175},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2015.2494218},
  abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  file = {/home/ert/Zotero/storage/ZUN3WGW8/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf;/home/ert/Zotero/storage/53UY3HJA/7352306.html},
  journal = {Proceedings of the IEEE},
  keywords = {Bayes methods,Big data,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,Linear programming,optimization,Optimization,response surface methodology,Statistical analysis,statistical learning},
  number = {1}
}

@article{shang2014_SurveyFunctionalPrincipal,
  title = {A Survey of Functional Principal Component Analysis},
  author = {Shang, Han Lin},
  year = {2014},
  month = apr,
  volume = {98},
  pages = {121--142},
  issn = {1863-818X},
  doi = {10.1007/s10182-013-0213-1},
  abstract = {Advances in data collection and storage have tremendously increased the presence of functional data, whose graphical representations are curves, images or shapes. As a new area of statistics, functional data analysis extends existing methodologies and theories from the realms of functional analysis, generalized linear model, multivariate data analysis, nonparametric statistics, regression models and many others. From both methodological and practical viewpoints, this paper provides a review of functional principal component analysis, and its use in explanatory analysis, modeling and forecasting, and classification of functional data.},
  file = {/home/ert/Zotero/storage/TANX55YW/Shang - 2014 - A survey of functional principal component analysi.pdf},
  journal = {AStA Adv Stat Anal},
  language = {en},
  number = {2}
}

@article{smith1936_PhotosynthesisRelationLight,
  title = {Photosynthesis in {{Relation}} to {{Light}} and {{Carbon Dioxide}}},
  author = {Smith, Emil L.},
  year = {1936},
  month = aug,
  volume = {22},
  pages = {504--511},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.22.8.504},
  chapter = {Physiology and Biochemistry},
  file = {/home/ert/Zotero/storage/XAUSFMJR/Smith - 1936 - Photosynthesis in Relation to Light and Carbon Dio.pdf;/home/ert/Zotero/storage/YIH9JBVS/504.html},
  journal = {PNAS},
  language = {en},
  number = {8},
  pmid = {16577734}
}

@article{wiqvistAcceleratingDelayedacceptanceMarkov2019,
  title = {Accelerating Delayed-Acceptance {{Markov}} Chain {{Monte Carlo}} Algorithms},
  author = {Wiqvist, Samuel and Picchini, Umberto and Forman, Julie Lyng and {Lindorff-Larsen}, Kresten and Boomsma, Wouter},
  year = {2019},
  month = may,
  abstract = {Delayed-acceptance Markov chain Monte Carlo (DA-MCMC) samples from a probability distribution via a two-stages version of the Metropolis-Hastings algorithm, by combining the target distribution with a "surrogate" (i.e. an approximate and computationally cheaper version) of said distribution. DA-MCMC accelerates MCMC sampling in complex applications, while still targeting the exact distribution. We design a computationally faster, albeit approximate, DA-MCMC algorithm. We consider parameter inference in a Bayesian setting where a surrogate likelihood function is introduced in the delayed-acceptance scheme. When the evaluation of the likelihood function is computationally intensive, our scheme produces a 2-4 times speed-up, compared to standard DA-MCMC. However, the acceleration is highly problem dependent. Inference results for the standard delayed-acceptance algorithm and our approximated version are similar, indicating that our algorithm can return reliable Bayesian inference. As a computationally intensive case study, we introduce a novel stochastic differential equation model for protein folding data.},
  archiveprefix = {arXiv},
  eprint = {1806.05982},
  eprinttype = {arxiv},
  file = {/home/ert/Zotero/storage/L9G5QTPE/Wiqvist et al. - 2019 - Accelerating delayed-acceptance Markov chain Monte.pdf;/home/ert/Zotero/storage/WS9S822F/1806.html},
  journal = {arXiv:1806.05982 [stat]},
  keywords = {Statistics - Computation},
  primaryclass = {stat}
}

@article{wuBayesianOptimizationGradients2018,
  title = {Bayesian {{Optimization}} with {{Gradients}}},
  author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew Gordon and Frazier, Peter I.},
  year = {2018},
  month = feb,
  abstract = {Bayesian optimization has been successful at global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.},
  archiveprefix = {arXiv},
  eprint = {1703.04389},
  eprinttype = {arxiv},
  file = {/home/ert/Zotero/storage/LTUPIN5K/Wu et al. - 2018 - Bayesian Optimization with Gradients.pdf;/home/ert/Zotero/storage/6DU5NIG6/1703.html},
  journal = {arXiv:1703.04389 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@article{wuSurrogateAcceleratedMulticanonical2016,
  title = {A Surrogate Accelerated Multicanonical {{Monte Carlo}} Method for Uncertainty Quantification},
  author = {Wu, Keyi and Li, Jinglai},
  year = {2016},
  month = sep,
  volume = {321},
  pages = {1098--1109},
  issn = {00219991},
  doi = {10.1016/j.jcp.2016.06.020},
  journal = {Journal of Computational Physics},
  language = {en}
}

@article{yanAdaptiveMultifidelityPolynomial2019,
  title = {Adaptive Multi-Fidelity Polynomial Chaos Approach to {{Bayesian}} Inference in Inverse Problems},
  author = {Yan, Liang and Zhou, Tao},
  year = {2019},
  month = mar,
  volume = {381},
  pages = {110--128},
  issn = {00219991},
  doi = {10.1016/j.jcp.2018.12.025},
  journal = {Journal of Computational Physics},
  language = {en}
}

@article{zhouAdaptiveKrigingSurrogate2018,
  title = {An Adaptive {{Kriging}} Surrogate Method for Efficient Joint Estimation of Hydraulic and Biochemical Parameters in Reactive Transport Modeling},
  author = {Zhou, Jun and Su, Xiaosi and Cui, Geng},
  year = {2018},
  month = sep,
  volume = {216},
  pages = {50--57},
  issn = {01697722},
  doi = {10.1016/j.jconhyd.2018.08.005},
  journal = {Journal of Contaminant Hydrology},
  language = {en}
}
